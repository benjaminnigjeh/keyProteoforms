{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "1ofexDX4dW-ApTNenyWkvqAK0XcAPUoyn",
      "authorship_tag": "ABX9TyMGVm0QRKNxkJbOC45m3bR7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminnigjeh/keyProteoforms/blob/main/Generalized_Transformer_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the pickled DataFrame\n",
        "df = pd.read_pickle('/content/drive/MyDrive/databank_learner2.pkl')\n",
        "\n"
      ],
      "metadata": {
        "id": "JDhNsDoOmZmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pronto"
      ],
      "metadata": {
        "id": "mmFkhT36mavN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pronto\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Load PSI-MOD ontology\n",
        "ontology = pronto.Ontology(\"/content/drive/MyDrive/PSI-MOD.obo\")\n",
        "\n",
        "def extract_mod_masses_and_locations(int_mod_str, n_term_str):\n",
        "    masses = []\n",
        "    locations = []\n",
        "\n",
        "    def parse_mods(mod_str, force_position=None):\n",
        "        mods = []\n",
        "        if not isinstance(mod_str, str) or not any(prefix in mod_str for prefix in (\"MOD:\", \"PSI-MOD:\")):\n",
        "            return mods\n",
        "\n",
        "        for mod_entry in mod_str.split('|'):\n",
        "            if '@' in mod_entry:\n",
        "                mod_part, suffix = mod_entry.split('@')\n",
        "                try:\n",
        "                    suffix = int(suffix)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "            else:\n",
        "                mod_part = mod_entry\n",
        "                suffix = 0\n",
        "\n",
        "            if ':' not in mod_part:\n",
        "                continue\n",
        "\n",
        "            prefix, number = mod_part.split(':')\n",
        "            if not number.isdigit():\n",
        "                continue\n",
        "\n",
        "            padded_number = number.zfill(5)\n",
        "            mod_id = f\"MOD:{padded_number}\"  # Always use MOD:xxxxx for lookup\n",
        "\n",
        "            if force_position is not None:\n",
        "                suffix = force_position\n",
        "\n",
        "            mods.append((mod_id, suffix))\n",
        "        return mods\n",
        "\n",
        "    mod_list = parse_mods(n_term_str, force_position=0) + parse_mods(int_mod_str)\n",
        "\n",
        "    for mod_id, location in mod_list:\n",
        "        locations.append(location)\n",
        "        try:\n",
        "            xrefs = ontology[mod_id].xrefs\n",
        "            diffavg_desc = next((x.description for x in xrefs if x.id == \"DiffAvg:\"), None)\n",
        "            masses.append(float(diffavg_desc))\n",
        "        except (KeyError, TypeError, ValueError):\n",
        "            masses.append(0.0)\n",
        "\n",
        "    return masses, locations\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def create_tertiary_mask_with_mass_shifts(canonical_seq, t_seq, ptm_locs, mass_shifts):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "    - mask: torch.LongTensor of shape [len(canonical_seq)], values in {0, 1, 2}\n",
        "    - mass_vector: torch.FloatTensor of shape [len(canonical_seq)], nonzero only at PTM sites\n",
        "    \"\"\"\n",
        "    if len(ptm_locs) != len(mass_shifts):\n",
        "        raise ValueError(f\"Mismatch: {len(ptm_locs)} PTM locations vs {len(mass_shifts)} mass shifts.\")\n",
        "\n",
        "    seq_len = len(canonical_seq)\n",
        "    mask = torch.zeros(seq_len, dtype=torch.long)         # ensure long for CE loss\n",
        "    mass_vector = torch.zeros(seq_len, dtype=torch.float32)\n",
        "\n",
        "    start_idx = canonical_seq.find(t_seq)\n",
        "    if start_idx == -1:\n",
        "        raise ValueError(f\"Truncated sequence '{t_seq}' not found in canonical sequence.\")\n",
        "\n",
        "    # Mark entire truncated region as \"truncated\" = 1\n",
        "    mask[start_idx:start_idx + len(t_seq)] = 1\n",
        "\n",
        "    # Mark PTM positions as 2 and assign their mass shifts\n",
        "    for abs_idx, shift in zip(ptm_locs, mass_shifts):\n",
        "        if not (0 <= abs_idx < seq_len):\n",
        "            raise IndexError(f\"PTM index {abs_idx} out of bounds for sequence length {seq_len}\")\n",
        "        mask[abs_idx] = 2\n",
        "        mass_vector[abs_idx] = shift\n",
        "\n",
        "    return mask, mass_vector\n",
        "\n",
        "\n",
        "class PTMDataset(Dataset):\n",
        "    def __init__(self, spectra, labels, deltas):\n",
        "        assert len(spectra) == len(labels) == len(deltas), \"All input lists must be the same length.\"\n",
        "\n",
        "        self.spectra = [\n",
        "            s if isinstance(s, torch.Tensor) else torch.tensor(np.array(s), dtype=torch.float32)\n",
        "            for s in spectra\n",
        "        ]\n",
        "        self.labels = labels\n",
        "        self.deltas = deltas\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.spectra)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.spectra[idx], self.labels[idx], self.deltas[idx]\n"
      ],
      "metadata": {
        "id": "bmXCOUeemdrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spectra = []\n",
        "labels = []\n",
        "deltas = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if df['Uniprot ID'][i] == 'HBB_HUMAN':\n",
        "        spec = df['cast spectra'][i]\n",
        "        canonical_seq = \"MVHLTPEEKSAVTALWGKVNVDEVGGEALGRLLVVYPWTQRFFESFGDLSTPDAVMGNPKVKAHGKKVLGAFSDGLAHLDNLKGTFATLSELHCDKLHVDPENFRLLGNVLVCVLAHHFGKEFTPPVQAAYQKVVAGVANALAHKYH\"\n",
        "        t_seq = df['sequence'][i]\n",
        "        int_mod_str = df['int_mod'][i]\n",
        "        n_term_str = df['n_term'][i]\n",
        "\n",
        "        try:\n",
        "            masses, locations = extract_mod_masses_and_locations(int_mod_str, n_term_str)\n",
        "\n",
        "            # Filter for only modifications within the canonical sequence\n",
        "            valid_idxs = [i for i, loc in enumerate(locations) if 0 <= loc < len(canonical_seq)]\n",
        "            ptm_locs = [locations[i] for i in valid_idxs]\n",
        "            ptm_masses = [masses[i] for i in valid_idxs]\n",
        "\n",
        "            mask, mass_vec = create_tertiary_mask_with_mass_shifts(canonical_seq, t_seq, ptm_locs, ptm_masses)\n",
        "\n",
        "            spectra.append(spec)\n",
        "            labels.append(mask)\n",
        "            deltas.append(mass_vec)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping index {i} due to error: {e}\")\n",
        "\n",
        "dataset = PTMDataset(spectra, labels, deltas)\n"
      ],
      "metadata": {
        "id": "hDXlx3cYmh9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Example: 80% training, 20% testing\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "VY1Qm8p5mmTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# --- Positional Encoding ---\n",
        "class PositionalEncoding1D(nn.Module):\n",
        "    def __init__(self, d_model, max_len=147):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# --- Binary PTM Site Classifier ---\n",
        "class PTMSiteClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1600, seq_len=147):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, 256)\n",
        "        self.encoder_pos_enc = PositionalEncoding1D(256, max_len=seq_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "        self.head = nn.Linear(256, 1)  # Binary classifier\n",
        "\n",
        "    def forward(self, spectra):\n",
        "        x = self.input_proj(spectra).unsqueeze(1).repeat(1, 147, 1)\n",
        "        x = self.encoder_pos_enc(x)\n",
        "        x = self.encoder(x)\n",
        "        return self.head(x).squeeze(-1)  # [B, 147]\n",
        "\n",
        "# --- Focal Loss for Binary Classification ---\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.9, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        bce = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
        "        probas = torch.sigmoid(logits)\n",
        "        pt = probas * targets + (1 - probas) * (1 - targets)\n",
        "        focal = self.alpha * (1 - pt) ** self.gamma * bce\n",
        "        return focal.mean() if self.reduction == \"mean\" else focal.sum()\n",
        "\n",
        "# --- Training ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PTMSiteClassifier().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = FocalLoss(alpha=0.9, gamma=2.0)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_true = [], []\n",
        "\n",
        "    for spectra, status_labels, _ in train_loader:\n",
        "        spectra = spectra.to(DEVICE)\n",
        "        is_modified = (status_labels == 2).float().to(DEVICE)  # Binary target\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(spectra)  # [B, 147]\n",
        "        loss = loss_fn(logits, is_modified)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "        all_preds.extend(preds.cpu().numpy().ravel())\n",
        "        all_true.extend(is_modified.cpu().numpy().ravel())\n",
        "\n",
        "    precision = precision_score(all_true, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_true, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_true, all_preds, zero_division=0)\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f} | Precision = {precision:.4f} | Recall = {recall:.4f} | F1 = {f1:.4f}\")\n",
        "\n",
        "# --- Evaluation on Test Set ---\n",
        "model.eval()\n",
        "all_preds, all_true = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectra, status_labels, _ in test_loader:\n",
        "        spectra = spectra.to(DEVICE)\n",
        "        is_modified = (status_labels == 2).float().to(DEVICE)\n",
        "        logits = model(spectra)\n",
        "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy().ravel())\n",
        "        all_true.extend(is_modified.cpu().numpy().ravel())\n",
        "\n",
        "# --- Metrics & Confusion Matrix ---\n",
        "precision = precision_score(all_true, all_preds, zero_division=0)\n",
        "recall = recall_score(all_true, all_preds, zero_division=0)\n",
        "f1 = f1_score(all_true, all_preds, zero_division=0)\n",
        "print(f\"\\n[TEST] Precision = {precision:.4f} | Recall = {recall:.4f} | F1 = {f1:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(all_true, all_preds, labels=[0, 1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Modified\", \"Modified\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Test Set - PTM Site Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v_EvY09Omq3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# --- Positional Encoding ---\n",
        "class PositionalEncoding1D(nn.Module):\n",
        "    def __init__(self, d_model, max_len=147):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# --- Delta Mass Regressor ---\n",
        "class DeltaMassRegressor(nn.Module):\n",
        "    def __init__(self, input_dim=1600, seq_len=147):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, 256)\n",
        "        self.encoder_pos_enc = PositionalEncoding1D(256, max_len=seq_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, dim_feedforward=512, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
        "        self.regressor = nn.Linear(256, 1)  # Predict delta mass\n",
        "\n",
        "    def forward(self, spectra):\n",
        "        x = self.input_proj(spectra).unsqueeze(1).repeat(1, 147, 1)\n",
        "        x = self.encoder_pos_enc(x)\n",
        "        x = self.encoder(x)\n",
        "        return self.regressor(x).squeeze(-1)  # [B, 147]\n",
        "\n",
        "# --- Training ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeltaMassRegressor().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_true = [], []\n",
        "\n",
        "    for spectra, status_labels, delta_labels in train_loader:\n",
        "        spectra = spectra.to(DEVICE)\n",
        "        is_modified = (status_labels == 2)\n",
        "        delta_labels = delta_labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred_deltas = model(spectra)  # [B, 147]\n",
        "\n",
        "        # Only compute loss on modified positions\n",
        "        loss = loss_fn(pred_deltas[is_modified], delta_labels[is_modified]) if is_modified.any() else torch.tensor(0.0, device=DEVICE)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        all_preds.extend(pred_deltas[is_modified].detach().cpu().numpy())\n",
        "        all_true.extend(delta_labels[is_modified].cpu().numpy())\n",
        "\n",
        "    mae = mean_absolute_error(all_true, all_preds) if all_true else 0.0\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f} | MAE on PTMs = {mae:.4f}\")\n",
        "\n",
        "# --- Evaluation on Test Set ---\n",
        "import seaborn as sns\n",
        "\n",
        "# --- Evaluation on Test Set ---\n",
        "model.eval()\n",
        "all_preds, all_true = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectra, status_labels, delta_labels in test_loader:\n",
        "        spectra = spectra.to(DEVICE)\n",
        "        is_modified = (status_labels == 2)\n",
        "        delta_labels = delta_labels.to(DEVICE)\n",
        "        pred_deltas = model(spectra)\n",
        "\n",
        "        all_preds.extend(pred_deltas[is_modified].cpu().numpy())\n",
        "        all_true.extend(delta_labels[is_modified].cpu().numpy())\n",
        "\n",
        "mae = mean_absolute_error(all_true, all_preds) if all_true else 0.0\n",
        "print(f\"\\n[TEST] Delta Mass MAE on PTM Sites = {mae:.4f}\")\n",
        "\n",
        "# --- Density Plot: True vs Predicted ---\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.kdeplot(x=all_true, y=all_preds, fill=True, cmap=\"mako\", thresh=0.01)\n",
        "plt.xlabel(\"True Delta Mass\")\n",
        "plt.ylabel(\"Predicted Delta Mass\")\n",
        "plt.title(\"Delta Mass KDE Plot (PTM Sites)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HQv_zCSpmqp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- XLSTM-based PTM Site Classifier ---\n",
        "class XLSTM_PTMSiteClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1600, seq_len=147, hidden_dim=256, num_layers=2, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(hidden_dim * 2)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim * 2, 1)  # Binary classifier\n",
        "        )\n",
        "\n",
        "    def forward(self, spectra):\n",
        "        x = self.input_proj(spectra)  # [B, 1600] → [B, hidden]\n",
        "        x = x.unsqueeze(1).repeat(1, self.seq_len, 1)  # [B, seq_len, hidden]\n",
        "        x, _ = self.lstm(x)  # [B, seq_len, hidden*2]\n",
        "        x = self.norm(x)\n",
        "        return self.classifier(x).squeeze(-1)  # [B, seq_len]\n",
        "\n",
        "# --- Focal Loss ---\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.9, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
        "        probas = torch.sigmoid(logits)\n",
        "        pt = probas * targets + (1 - probas) * (1 - targets)\n",
        "        focal = self.alpha * (1 - pt) ** self.gamma * bce\n",
        "        return focal.mean() if self.reduction == \"mean\" else focal.sum()\n",
        "\n",
        "# --- Training Loop ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = XLSTM_PTMSiteClassifier().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = FocalLoss(alpha=0.9, gamma=2.0)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_true = [], []\n",
        "\n",
        "    for spectra, status_labels, _ in train_loader:\n",
        "        spectra = spectra.to(DEVICE)\n",
        "        is_modified = (status_labels == 2).float().to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(spectra)  # [B, 147]\n",
        "        loss = loss_fn(logits, is_modified)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "        all_preds.extend(preds.cpu().numpy().ravel())\n",
        "        all_true.extend(is_modified.cpu().numpy().ravel())\n",
        "\n",
        "    precision = precision_score(all_true, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_true, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_true, all_preds, zero_division=0)\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f} | Precision = {precision:.4f} | Recall = {recall:.4f} | F1 = {f1:.4f}\")\n",
        "\n",
        "# --- Evaluation ---\n",
        "model.eval()\n",
        "all_preds, all_true = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectra, status_labels, _ in test_loader:\n",
        "        spectra = spectra.to(DEVICE)\n",
        "        is_modified = (status_labels == 2).float().to(DEVICE)\n",
        "        logits = model(spectra)\n",
        "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy().ravel())\n",
        "        all_true.extend(is_modified.cpu().numpy().ravel())\n",
        "\n",
        "precision = precision_score(all_true, all_preds, zero_division=0)\n",
        "recall = recall_score(all_true, all_preds, zero_division=0)\n",
        "f1 = f1_score(all_true, all_preds, zero_division=0)\n",
        "print(f\"\\n[TEST] Precision = {precision:.4f} | Recall = {recall:.4f} | F1 = {f1:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(all_true, all_preds, labels=[0, 1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Modified\", \"Modified\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Test Set - PTM Site Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "STbyIAjKv_MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Positional Encoding ---\n",
        "class PositionalEncoding1D(nn.Module):\n",
        "    def __init__(self, d_model, max_len=147):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# --- Hybrid Model: BiLSTM + Transformer ---\n",
        "class Hybrid_PTMSiteClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=1600, seq_len=147, hidden_dim=256, lstm_layers=1, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=hidden_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding1D(d_model=hidden_dim*2, max_len=seq_len)\n",
        "\n",
        "        transformer_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim*2,\n",
        "            nhead=8,\n",
        "            dim_feedforward=512,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(transformer_layer, num_layers=2)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim*2, 1)  # Binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, spectra):\n",
        "        x = self.input_proj(spectra)  # [B, 1600] → [B, hidden]\n",
        "        x = x.unsqueeze(1).repeat(1, self.seq_len, 1)  # [B, 147, hidden]\n",
        "        x, _ = self.bilstm(x)  # [B, 147, hidden*2]\n",
        "        x = self.pos_encoder(x)  # Add positional info\n",
        "        x = self.transformer(x)  # Contextualize globally\n",
        "        return self.classifier(x).squeeze(-1)  # [B, 147]\n",
        "\n",
        "# --- Focal Loss ---\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.9, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
        "        probas = torch.sigmoid(logits)\n",
        "        pt = probas * targets + (1 - probas) * (1 - targets)\n",
        "        focal = self.alpha * (1 - pt) ** self.gamma * bce\n",
        "        return focal.mean() if self.reduction == \"mean\" else focal.sum()\n",
        "\n",
        "# --- Training ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Hybrid_PTMSiteClassifier().to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "loss_fn = FocalLoss(alpha=0.9, gamma=2.0)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_preds, all_true = [], []\n",
        "\n",
        "    for spectra, status_labels, _ in train_loader:\n",
        "        spectra = spectra.to(DEVICE)\n",
        "        is_modified = (status_labels == 2).float().to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(spectra)  # [B, 147]\n",
        "        loss = loss_fn(logits, is_modified)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "        all_preds.extend(preds.cpu().numpy().ravel())\n",
        "        all_true.extend(is_modified.cpu().numpy().ravel())\n",
        "\n",
        "    precision = precision_score(all_true, all_preds, zero_division=0)\n",
        "    recall = recall_score(all_true, all_preds, zero_division=0)\n",
        "    f1 = f1_score(all_true, all_preds, zero_division=0)\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss/len(train_loader):.4f} | Precision = {precision:.4f} | Recall = {recall:.4f} | F1 = {f1:.4f}\")\n",
        "\n",
        "# --- Evaluation ---\n",
        "model.eval()\n",
        "all_preds, all_true = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for spectra, status_labels, _ in test_loader:\n",
        "        spectra = spectra.to(DEVICE)\n",
        "        is_modified = (status_labels == 2).float().to(DEVICE)\n",
        "        logits = model(spectra)\n",
        "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy().ravel())\n",
        "        all_true.extend(is_modified.cpu().numpy().ravel())\n",
        "\n",
        "precision = precision_score(all_true, all_preds, zero_division=0)\n",
        "recall = recall_score(all_true, all_preds, zero_division=0)\n",
        "f1 = f1_score(all_true, all_preds, zero_division=0)\n",
        "print(f\"\\n[TEST] Precision = {precision:.4f} | Recall = {recall:.4f} | F1 = {f1:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(all_true, all_preds, labels=[0, 1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Not Modified\", \"Modified\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Test Set - PTM Site Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wipoPwXvwBDy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}