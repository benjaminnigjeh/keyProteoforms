{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKH7GH/xdY6stiIF6m1+BZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminnigjeh/keyProteoforms/blob/main/VAEdatasetGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Installation of external packages"
      ],
      "metadata": {
        "id": "wzB_pVLnxcPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y -qq mono-complete\n",
        "!pip install -qq fisher-py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zsNbLmGxK-h",
        "outputId": "b818fad0-db14-4fa3-d2e7-0c0ad4ae086f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import packages"
      ],
      "metadata": {
        "id": "MK_jcT0ox1FW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXZuzWN3rzEC"
      },
      "outputs": [],
      "source": [
        "from fisher_py.data.business import Scan\n",
        "from fisher_py import RawFile\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.signal import find_peaks\n",
        "from scipy.optimize import curve_fit\n",
        "import subprocess\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper functions"
      ],
      "metadata": {
        "id": "jMWCiMJYx7Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantitativeDeconvolution(folder_path, file_path):\n",
        "    os.chdir(folder_path)\n",
        "    def helper_regex(text):\n",
        "      match = re.search(rf\"{'Full'}\\s+(\\w+)\", text)\n",
        "      if match:\n",
        "        return match.group(1)\n",
        "      return None\n",
        "    raw = RawFile(file_path)\n",
        "    dir_name = file_path.split('/')[-1].split('.')[0]\n",
        "    new_dir = os.path.join(os.getcwd(), dir_name)\n",
        "    if not os.path.exists(new_dir): os.mkdir(new_dir)\n",
        "    os.chdir(new_dir)\n",
        "    scans = []\n",
        "    for i in range(1, raw.number_of_scans):\n",
        "            raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
        "            if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
        "                    scans.append(i)\n",
        "    for i in tqdm(scans):\n",
        "        raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
        "        scan_masses = raw_scan.preferred_masses\n",
        "        scan_intensities = raw_scan.preferred_intensities\n",
        "        data = {\n",
        "                'Column1': scan_masses,\n",
        "                'Column2': scan_intensities}\n",
        "        df = pd.DataFrame(data)\n",
        "        file_name = str(i) + '.txt'\n",
        "        df.to_csv(file_name, sep='\\t', index=False)\n",
        "        command = f\"python -m unidec -f {file_name}\"\n",
        "        subprocess.run(command, shell=True)\n",
        "\n",
        "    x_value = list(range(5000, 21000, 10))\n",
        "    y_value = [0]*len(x_value)\n",
        "    for i in scans:\n",
        "        print(i)\n",
        "        os.chdir(new_dir)\n",
        "        os.chdir(str(i) + '_unidecfiles')\n",
        "        try:\n",
        "            data = np.loadtxt(str(i) + '_mass.txt')\n",
        "            x = data[:, 0]  # First column (x-values)\n",
        "            y = data[:, 1]  # Second column (y-values)\n",
        "            for j in range(0, len(x)):\n",
        "                 if x[j] in x_value:\n",
        "                      index = x_value.index(x[j])\n",
        "                      if index in range(len(y_value)):\n",
        "                           y_value[index] = y_value[index] + y[j]\n",
        "\n",
        "        except FileNotFoundError: print(\"File not found. Skipping...\")\n",
        "    data = {\n",
        "                'Column1': x_value,\n",
        "                'Column2': y_value}\n",
        "    df = pd.DataFrame(data)\n",
        "    os.chdir(folder_path)\n",
        "    file_name = file_path[:-4] + '.txt'\n",
        "    df.to_csv(file_name, sep='\\t', index=False)\n",
        "\n",
        "    return()\n",
        "\n",
        "def MS1quan(folder_path, file_path):\n",
        "    os.chdir(folder_path)\n",
        "    def helper_regex(text):\n",
        "      match = re.search(rf\"{'Full'}\\s+(\\w+)\", text)\n",
        "      if match:\n",
        "        return match.group(1)\n",
        "      return None\n",
        "    raw = RawFile(file_path)\n",
        "    data_intensities = [0]*1369\n",
        "    for i in tqdm(range(1, raw.number_of_scans)):\n",
        "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
        "                if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
        "                        scan_masses = raw_scan.preferred_masses\n",
        "                        scan_intensities = raw_scan.preferred_intensities\n",
        "                        for j in range(0,len(scan_masses)):\n",
        "                                index = round(scan_masses[j])\n",
        "                                if index > 600 and index < 1969:\n",
        "                                        data_intensities[index-600] = scan_intensities[j] + data_intensities[index-600]\n",
        "    return(data_intensities)"
      ],
      "metadata": {
        "id": "8zQj5uZEx6Zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quantitative deconvolution: scan-by-scan deconvolution"
      ],
      "metadata": {
        "id": "qZiDg2rwyQw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = 'D:/conv/samples'\n",
        "os.chdir(folder_path)\n",
        "\n",
        "file_path_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "for file_path in file_path_list:\n",
        "    quantitativeDeconvolution(folder_path, file_path)\n"
      ],
      "metadata": {
        "id": "nSjdDKDuyQRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quantitative deconvolution: data aggregation"
      ],
      "metadata": {
        "id": "Y5LlDs3_y7B8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = 'D:/conv/results'\n",
        "os.chdir(folder_path)\n",
        "\n",
        "file_path_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "combined_result = []\n",
        "for file_txt in file_path_list:\n",
        "    data = np.genfromtxt(str(file_txt))\n",
        "    combined_result.append(data[:, 1])"
      ],
      "metadata": {
        "id": "_bDkxreZy1m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quantitative deconvolution: data export"
      ],
      "metadata": {
        "id": "H8Ze5HYPu5Ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(combined_result)\n",
        "\n",
        "column_max = df.max()\n",
        "df_normalized = df / (column_max + 0.001)\n",
        "\n",
        "df_normalized.to_csv('D:/combined_result.csv')"
      ],
      "metadata": {
        "id": "IzV9MpZGu4cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MS1 quantification: data aggregation"
      ],
      "metadata": {
        "id": "6iiCxNZTzqPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = 'D:/conv/samples'\n",
        "os.chdir(folder_path)\n",
        "\n",
        "file_path_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "result = []\n",
        "\n",
        "for file_path in file_path_list[57]:\n",
        "    result.append(MS1quan(folder_path, file_path))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tBYS2Jqhzp9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MS1 quantification: data export"
      ],
      "metadata": {
        "id": "fo81XyvH0MLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(result)\n",
        "\n",
        "column_max = df.max()\n",
        "df_normalized = df / (column_max + 0.001)\n",
        "\n",
        "df_normalized.to_csv('D:/ms1_result.csv')"
      ],
      "metadata": {
        "id": "R9fVnGW-0Ive"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}