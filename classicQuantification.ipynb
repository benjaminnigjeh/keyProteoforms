{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu2YpYOBG1pjzC5eMJZWde",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminnigjeh/keyProteoforms/blob/main/classicQuantification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install required libraries"
      ],
      "metadata": {
        "id": "Hr04vrjhRqRR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gprKVrMPRVRl"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y mono-complete\n",
        "!pip install fisher-py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import external libraries"
      ],
      "metadata": {
        "id": "R6lUZV3TSZyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fisher_py.data.business import Scan\n",
        "from fisher_py import RawFile\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import interp1d\n",
        "from scipy.stats import linregress\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "qNGz1hLtSWZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DataBank functions"
      ],
      "metadata": {
        "id": "PjpDkiG1Sls8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wholeCasting(folder_path, cast_path):\n",
        "    os.chdir(folder_path)\n",
        "\n",
        "    def helper_regex(text):\n",
        "        match = re.search(rf\"{'Full'}\\s+(\\w+)\", text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        return None\n",
        "    def find_matching_keys(sequence: str, substring_dict: dict) -> list:\n",
        "        return [key for key, substrings in substring_dict.items() if any(substring in sequence for substring in substrings)]\n",
        "\n",
        "\n",
        "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "    substring_dict_sample = {\"Disease\": [\"PDAD\", \"AD\", \"PD\"], \"Normal Healthy\": [\"NC\"]}\n",
        "    substring_dict_prep = {\"Pellet\": [\"Pellet\"], \"Soluble\": [\"Soluble\"]}\n",
        "\n",
        "    file_name = []\n",
        "\n",
        "    scan_type = []\n",
        "    scan_number = []\n",
        "    retention_time = []\n",
        "    cast_spectra = []\n",
        "\n",
        "    mz_value = []\n",
        "\n",
        "    for raw_name in files:\n",
        "        raw = RawFile(raw_name)\n",
        "        print(raw_name)\n",
        "        for i in tqdm(range(1, raw.number_of_scans), desc=\"Processing scans\", ncols=100):\n",
        "            raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
        "            file_name.append(raw_name)\n",
        "\n",
        "            if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
        "                scan_type.append('MS1')\n",
        "                scan_number.append(raw_scan.scan_statistics.scan_number)\n",
        "                retention_time.append(raw.get_retention_time_from_scan_number(raw_scan.scan_statistics.scan_number))\n",
        "                mz_value.append('')\n",
        "\n",
        "                data_intensities = [0]*13690\n",
        "                scan_masses = raw_scan.preferred_masses\n",
        "                scan_intensities = raw_scan.preferred_intensities\n",
        "\n",
        "                for j in range(0,len(scan_masses)):\n",
        "                    index = int(round(scan_masses[j], 2)*10)\n",
        "                    if index > 6000 and index < 19360:\n",
        "                        data_intensities[index-6000] = scan_intensities[j] + data_intensities[index-6000]\n",
        "\n",
        "                cast_spectra.append(data_intensities)\n",
        "\n",
        "\n",
        "            if str(helper_regex(raw_scan.scan_type)) == 'ms2':\n",
        "                scan_type.append('MS2')\n",
        "                scan_number.append(raw_scan.scan_statistics.scan_number)\n",
        "                retention_time.append(raw.get_retention_time_from_scan_number(raw_scan.scan_statistics.scan_number))\n",
        "                mz_value.append(float(re.findall(r'[\\d]*[.][\\d]+', raw_scan.scan_type)[1]))\n",
        "\n",
        "                data_intensities = [0]*1600\n",
        "                scan_masses = raw_scan.preferred_masses\n",
        "                scan_intensities = raw_scan.preferred_intensities\n",
        "\n",
        "                for j in range(0,len(scan_masses)):\n",
        "                    index = round(scan_masses[j])\n",
        "                    if index > 400 and index < 2000:\n",
        "                        data_intensities[index-400] = scan_intensities[j] + data_intensities[index-400]\n",
        "                data_intensities = np.array(data_intensities)\n",
        "                max_value = np.max(data_intensities)\n",
        "                data_intensities_norm = data_intensities / max_value\n",
        "                data_intensities_norm = data_intensities_norm.astype(np.float16)\n",
        "                data_intensities_norm.tolist()\n",
        "                cast_spectra.append(data_intensities_norm)\n",
        "\n",
        "    scan_dict = {'sample_name': file_name, 'scan': scan_number,'scan_type': scan_type, 'retntion time': retention_time, 'm/z': mz_value, 'cast spectra': cast_spectra}\n",
        "\n",
        "    with open(cast_path, \"wb\") as f:\n",
        "        pickle.dump(scan_dict, f)\n",
        "\n",
        "    return()\n",
        "\n",
        "def ID_import(tdportal, databank, cast_path):\n",
        "  def str_to_int(st):\n",
        "      internal = []\n",
        "      digits = re.findall(r'\\d+', st)\n",
        "      for i in range(0, len(digits)):\n",
        "          internal.append(int(digits[i]))\n",
        "      return(internal)\n",
        "\n",
        "  scan_number = [0]*len(tdportal['File Name'])\n",
        "  td_samples = []\n",
        "\n",
        "  for i in range(0, len(tdportal['File Name'])):\n",
        "      scan_number[i] = str_to_int(str(tdportal['Fragment Scans'][i]))\n",
        "      if tdportal['File Name'][i] not in td_samples:\n",
        "        td_samples.append(tdportal['File Name'][i])\n",
        "\n",
        "  my_dic_scan = {key: [] for key in td_samples}\n",
        "  my_dic_index = {key: [] for key in td_samples}\n",
        "\n",
        "  for i in range(0, len(tdportal['File Name'])):\n",
        "      my_dic_scan[tdportal['File Name'][i]].append(scan_number[i])\n",
        "      my_dic_index[tdportal['File Name'][i]].append([i]*len(scan_number[i]))\n",
        "\n",
        "  for i in range(0, len(td_samples)):\n",
        "      nested_list = my_dic_scan[td_samples[i]]\n",
        "      flat_list = []\n",
        "      for item in nested_list:\n",
        "          if isinstance(item, list):\n",
        "              flat_list.extend(item)\n",
        "          else:\n",
        "              flat_list.append(item)\n",
        "      my_dic_scan[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "\n",
        "\n",
        "  for i in range(0, len(td_samples)):\n",
        "      nested_list = my_dic_index[td_samples[i]]\n",
        "      flat_list = []\n",
        "      for item in nested_list:\n",
        "          if isinstance(item, list):\n",
        "              flat_list.extend(item)\n",
        "          else:\n",
        "              flat_list.append(item)\n",
        "      my_dic_index[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "\n",
        "  sequence = []\n",
        "  MASS = []\n",
        "  Uniprot_ID = []\n",
        "  Accession = []\n",
        "\n",
        "  for i in tqdm(range(0, len(databank['scan'])), desc=\"Processing scans\", ncols=100):\n",
        "      if databank['scan'][i] in my_dic_scan[databank['sample_name'][i]]:\n",
        "          tt = my_dic_index[databank['sample_name'][i]][my_dic_scan[databank['sample_name'][i]].index(databank['scan'][i])]\n",
        "          sequence.append(tdportal['Sequence'][tt])\n",
        "          MASS.append(tdportal['Average Mass'][tt])\n",
        "          Uniprot_ID.append(tdportal['Uniprot Id'][tt])\n",
        "          Accession.append(tdportal['Accession'][tt])\n",
        "      else:\n",
        "          sequence.append('None')\n",
        "          MASS.append('None')\n",
        "          Uniprot_ID.append('None')\n",
        "          Accession.append('None')\n",
        "\n",
        "\n",
        "  databank['sequence'] = sequence\n",
        "  databank['MASS'] = MASS\n",
        "  databank['Uniprot ID'] = Uniprot_ID\n",
        "  databank['Accession'] = Accession\n",
        "\n",
        "  databank = pd.DataFrame(databank)\n",
        "\n",
        "  databank.to_hdf(cast_path, key=\"databank\", mode=\"w\")\n",
        "\n",
        "  return()\n"
      ],
      "metadata": {
        "id": "G4p2TwnOSeu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build Databank"
      ],
      "metadata": {
        "id": "SKfLcJgqSxkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wholeCasting(\"D:/joker/joker1162025/Samples/\",'D:/joker/databank')\n",
        "\n",
        "with open(\"D:/joker/databank\", \"rb\") as f:\n",
        "    databank = pickle.load(f)\n",
        "tdportal = pd.read_csv('D:/joker/hit_report.csv')\n",
        "cast_path = 'D:/joker/databank_updated'\n",
        "\n",
        "ID_import(tdportal, databank, cast_path)"
      ],
      "metadata": {
        "id": "XZDQ4kWlS1qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Align retention times"
      ],
      "metadata": {
        "id": "e0QMxKfKTcjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_hdf(\"D:/joker/databank_updated\", key=\"databank\")\n",
        "sample_list = df['sample_name'].unique()\n",
        "\n",
        "def engine(cast1, cast2):\n",
        "    X = [cast1, cast2]\n",
        "    return(round(cosine_similarity(X)[1][0], 2))\n",
        "\n",
        "ret_1 = []\n",
        "ret_2 = []\n",
        "\n",
        "df = df[df['scan_type'] == 'MS2'].reset_index(drop=True)\n",
        "df1 = df[df['sample_name'] == sample_list[0]].reset_index(drop=True)\n",
        "df2 = df[df['sample_name'] == sample_list[2]].reset_index(drop=True)\n",
        "\n",
        "for i in range(len(df1)):\n",
        "    print( i, 'from', len(df1))\n",
        "    for j in range(len(df2)):\n",
        "        if df1['m/z'][i] - 0.1 < df2['m/z'][j] < df1['m/z'][i] + 0.1:\n",
        "            a = engine(df1['cast spectra'][i], df2['cast spectra'][j])\n",
        "            if a > 0.99:\n",
        "                ret_1.append(df1['retntion time'][i])\n",
        "                ret_2.append(df2['retntion time'][j])\n",
        "\n",
        "# Zip the two lists together\n",
        "zipped = list(zip(ret_1, ret_2))\n",
        "\n",
        "# Sort by the first element (ret_1)\n",
        "zipped_sorted = sorted(zipped, key=lambda x: x[0])\n",
        "\n",
        "# Unzip back into separate lists\n",
        "ret_1_sorted, ret_2_sorted = zip(*zipped_sorted)\n",
        "\n",
        "# Optional: convert back to lists\n",
        "ret_1_sorted = list(ret_1_sorted)\n",
        "ret_2_sorted = list(ret_2_sorted)\n",
        "\n",
        "# Convert lists to pandas Series\n",
        "ret_1_series = pd.Series(ret_1_sorted)\n",
        "ret_2_series = pd.Series(ret_2_sorted)\n",
        "\n",
        "# Apply rolling average with window of 20\n",
        "ret_1_smooth = ret_1_series.rolling(window=10, center=True).mean()\n",
        "ret_2_smooth = ret_2_series.rolling(window=10, center=True).mean()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(ret_1_smooth, ret_2_smooth, '.', alpha=0.7, color='mediumseagreen')\n",
        "plt.xlabel(sample_list[0])\n",
        "plt.ylabel(sample_list[2])\n",
        "#plt.title('Smoothed Retention Time Comparison')\n",
        "plt.grid(True)\n",
        "#plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Make sure ret_1_smooth and ret_2_smooth are numpy arrays and drop NaNs\n",
        "valid = (~pd.isna(ret_1_smooth)) & (~pd.isna(ret_2_smooth))\n",
        "x_vals = np.array(ret_1_smooth[valid])\n",
        "y_vals = np.array(ret_2_smooth[valid])\n",
        "\n",
        "# Create interpolation function (linear, but you can choose 'cubic' etc.)\n",
        "interpolator = interp1d(x_vals, y_vals, kind='linear', fill_value=\"extrapolate\")\n",
        "\n",
        "def get_ret2_from_ret1(x):\n",
        "    return interpolator(x)\n",
        "\n",
        "x_query = 100  # or a list/array of x values\n",
        "y_result = get_ret2_from_ret1(x_query)\n",
        "print(f\"Interpolated value at x={x_query}: y={y_result}\")\n"
      ],
      "metadata": {
        "id": "w-YC7pXOTg8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Find unique features"
      ],
      "metadata": {
        "id": "KU_qDTnMUce4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['scan_type']=='MS2']\n",
        "\n",
        "mzvalues = df['m/z']\n",
        "retvalues = df['retntion time']\n",
        "idvalues = df['Uniprot ID']\n",
        "massvalues = df['MASS']\n",
        "\n",
        "# Zip the two lists together\n",
        "zipped = list(zip(retvalues, mzvalues, idvalues, massvalues))\n",
        "\n",
        "# Sort by the first element (ret_1)\n",
        "zipped_sorted = sorted(zipped, key=lambda x: x[0])\n",
        "\n",
        "# Unzip back into separate lists\n",
        "ret_sorted, mz_sorted, id_sorted, mass_sorted = zip(*zipped_sorted)\n",
        "\n",
        "# Optional: convert back to lists\n",
        "ret_sorted = list(ret_sorted)\n",
        "mz_sorted = list(mz_sorted)\n",
        "id_sorted = list(id_sorted)\n",
        "mass_sorted = list(mass_sorted)\n",
        "\n",
        "tolerance = 1\n",
        "window = 10\n",
        "feature_mz = []\n",
        "feature_ret = []\n",
        "feature_id = []\n",
        "feature_mass = []\n",
        "\n",
        "for i in range(0, len(ret_sorted)):\n",
        "    print(i, 'from', len(ret_sorted))\n",
        "    if 600 < mz_sorted[i] < 1396 and ret_sorted[i] > 12:\n",
        "        indexes = [index for index, value in enumerate(feature_mz) if abs(value - mz_sorted[i]) <= tolerance]\n",
        "        counter = 0\n",
        "        for j in indexes:\n",
        "            if abs(feature_ret[j] - ret_sorted[i]) <= window:\n",
        "                counter = counter + 1\n",
        "        if counter == 0:\n",
        "            print('unique')\n",
        "            feature_mz.append(mz_sorted[i])\n",
        "            feature_ret.append(ret_sorted[i])\n",
        "            feature_id.append(id_sorted[i])\n",
        "            feature_mass.append(mass_sorted[i])"
      ],
      "metadata": {
        "id": "YLkgYvCyUiXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Quantification"
      ],
      "metadata": {
        "id": "L3eezkVsUxJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = sample_list\n",
        "df = pd.read_hdf(\"D:/joker/databank_updated\", key=\"databank\")\n",
        "\n",
        "def quantification(df, sample_number):\n",
        "    df_all = df.copy()\n",
        "    samples = df_all['sample_name'].unique()\n",
        "    df = df_all[df_all['scan_type'] == 'MS1'].reset_index(drop=True)\n",
        "    df_sample = df[df['sample_name'] == samples[sample_number]].reset_index(drop=True)\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(feature_mz)):\n",
        "        print( i, 'from', len(feature_mz))\n",
        "        a = []\n",
        "        for j in range(0, len(df_sample)):\n",
        "            if feature_ret[i] - 4 < df_sample['retntion time'][j] < feature_ret[i] + 4:\n",
        "                a.append(df_sample['cast spectra'][j][int(round(feature_mz[i], 2)*10) - 6000])\n",
        "        results.append(np.mean(a))\n",
        "    return(results)"
      ],
      "metadata": {
        "id": "TUqXod0uUzZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CSV output"
      ],
      "metadata": {
        "id": "Hp5sAVKV14-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = df.copy()\n",
        "\n",
        "df1 = df_all[df_all['scan_type'] == 'MS1'].reset_index(drop=True)\n",
        "samples = df1['sample_name'].unique()\n",
        "\n",
        "results = []\n",
        "for i in range(0, len(samples)):\n",
        "    results.append(quantification(df, i))\n",
        "\n",
        "output = pd.DataFrame(results)\n",
        "\n",
        "merged = [f\"{a}:{b}:{c}\" for a, b, c in zip(feature_id, feature_mass, feature_mz)]\n",
        "\n",
        "output.columns= merged\n",
        "\n",
        "output.insert(0, 'sample_name', samples)\n",
        "\n",
        "output.to_csv('D:/joker/result.csv')"
      ],
      "metadata": {
        "id": "VSYXh2CG13N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Large scale visualization"
      ],
      "metadata": {
        "id": "zW2AxV6-U5aJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(0, 6):\n",
        "    for j in range(0, 6):\n",
        "        name = str(i)+str(j)+'.jpeg'\n",
        "\n",
        "        results_1 = quantification(df, i)\n",
        "        results_2 = quantification(df, j)\n",
        "\n",
        "        # Create the scatter plot\n",
        "        plt.scatter(results_1, results_2, label='Data points')\n",
        "\n",
        "        # Perform linear regression\n",
        "        slope, intercept, r_value, p_value, std_err = linregress(results_1, results_2)\n",
        "\n",
        "        # Create the linear regression line\n",
        "        regression_line = np.array(results_1) * slope + intercept\n",
        "\n",
        "        # Plot the regression line\n",
        "        plt.plot(results_1, regression_line, color='red', label=f'Linear fit: $R^2$ = {r_value**2:.2f}')\n",
        "\n",
        "        # Add labels and a title\n",
        "        plt.xlabel(samples[i])  # Label for the x-axis\n",
        "        plt.ylabel(samples[j])  # Label for the y-axis\n",
        "        #plt.title('Quantification with Linear Regression')  # Title of the plot\n",
        "\n",
        "        # Show the legend\n",
        "        plt.legend()\n",
        "        plt.savefig(name)\n",
        "        # Show the plot\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "M5Q5N0JNVHy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}