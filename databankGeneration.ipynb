{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3R99nK-8j4dE"
      ],
      "mount_file_id": "1X8yo2UI7EEgTRg11f7GTaA_D2F-AFQkc",
      "authorship_tag": "ABX9TyNzXCw0BOaRAI8qQdSESTiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminnigjeh/keyProteoforms/blob/main/databankGeneration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Fisher-py module\n",
        "This is a module for parsing Thermo raw files. Since it's run time is pythonnet, it requires installation of .NET framework."
      ],
      "metadata": {
        "id": "6QCFIUVefrKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y mono-complete\n",
        "!pip install fisher-py"
      ],
      "metadata": {
        "id": "vJ1Z4iWWe42d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import external libraries"
      ],
      "metadata": {
        "id": "l19s1jqSgcdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fisher_py.data.business import Scan\n",
        "from fisher_py import RawFile\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "aFX1YSz_c-sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Function to build Databank"
      ],
      "metadata": {
        "id": "3R99nK-8j4dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wholeCasting(folder_path, cast_path):\n",
        "    os.chdir(folder_path)\n",
        "\n",
        "    def helper_regex(text):\n",
        "        match = re.search(rf\"{'Full'}\\s+(\\w+)\", text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        return None\n",
        "    def find_matching_keys(sequence: str, substring_dict: dict) -> list:\n",
        "        return [key for key, substrings in substring_dict.items() if any(substring in sequence for substring in substrings)]\n",
        "\n",
        "\n",
        "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "    substring_dict_sample = {\"Disease\": [\"PDAD\", \"AD\", \"PD\"], \"Normal Healthy\": [\"NC\"]}\n",
        "    substring_dict_prep = {\"Pellet\": [\"Pellet\"], \"Soluble\": [\"Soluble\"]}\n",
        "\n",
        "    file_name = []\n",
        "    sample_group = []\n",
        "    prep_group = []\n",
        "\n",
        "    scan_type = []\n",
        "    scan_number = []\n",
        "    retention_time = []\n",
        "    cast_spectra = []\n",
        "\n",
        "    mz_value = []\n",
        "\n",
        "    for raw_name in files:\n",
        "        raw = RawFile(raw_name)\n",
        "        print(raw_name)\n",
        "        for i in tqdm(range(1, raw.number_of_scans), desc=\"Processing scans\", ncols=100):\n",
        "            raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
        "            file_name.append(raw_name)\n",
        "            sample_group.append(find_matching_keys(raw_name, substring_dict_sample)[0])\n",
        "            prep_group.append(find_matching_keys(raw_name, substring_dict_prep)[0])\n",
        "\n",
        "            if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
        "                scan_type.append('MS1')\n",
        "                scan_number.append(raw_scan.scan_statistics.scan_number)\n",
        "                retention_time.append(raw.get_retention_time_from_scan_number(raw_scan.scan_statistics.scan_number))\n",
        "                mz_value.append('')\n",
        "\n",
        "                data_intensities = [0]*13690\n",
        "                scan_masses = raw_scan.preferred_masses\n",
        "                scan_intensities = raw_scan.preferred_intensities\n",
        "\n",
        "                for j in range(0,len(scan_masses)):\n",
        "                    index = int(round(scan_masses[j], 2)*10)\n",
        "                    if index > 6000 and index < 19360:\n",
        "                        data_intensities[index-6000] = scan_intensities[j] + data_intensities[index-6000]\n",
        "\n",
        "                cast_spectra.append(data_intensities)\n",
        "\n",
        "\n",
        "            if str(helper_regex(raw_scan.scan_type)) == 'ms2':\n",
        "                scan_type.append('MS2')\n",
        "                scan_number.append(raw_scan.scan_statistics.scan_number)\n",
        "                retention_time.append(raw.get_retention_time_from_scan_number(raw_scan.scan_statistics.scan_number))\n",
        "                mz_value.append(float(re.findall(r'[\\d]*[.][\\d]+', raw_scan.scan_type)[1]))\n",
        "\n",
        "                data_intensities = [0]*1600\n",
        "                scan_masses = raw_scan.preferred_masses\n",
        "                scan_intensities = raw_scan.preferred_intensities\n",
        "\n",
        "                for j in range(0,len(scan_masses)):\n",
        "                    index = round(scan_masses[j])\n",
        "                    if index > 400 and index < 2000:\n",
        "                        data_intensities[index-400] = scan_intensities[j] + data_intensities[index-400]\n",
        "                data_intensities = np.array(data_intensities)\n",
        "                max_value = np.max(data_intensities)\n",
        "                data_intensities_norm = data_intensities / max_value\n",
        "                data_intensities_norm = data_intensities_norm.astype(np.float16)\n",
        "                data_intensities_norm.tolist()\n",
        "                cast_spectra.append(data_intensities_norm)\n",
        "\n",
        "    scan_dict = {'sample_name': file_name, 'group_name': sample_group, 'sample_prep': prep_group, 'scan': scan_number,'scan_type': scan_type, 'retntion time': retention_time, 'm/z': mz_value, 'cast spectra': cast_spectra}\n",
        "\n",
        "    with open(cast_path, \"wb\") as f:\n",
        "        pickle.dump(scan_dict, f)\n",
        "\n",
        "    return()"
      ],
      "metadata": {
        "id": "UlnFvYGwcquy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Function to incorporate tdporal report into Databank"
      ],
      "metadata": {
        "id": "xDKk8aLD830V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ID_import(tdportal, databank, cast_path):\n",
        "  def str_to_int(st):\n",
        "      internal = []\n",
        "      digits = re.findall(r'\\d+', st)\n",
        "      for i in range(0, len(digits)):\n",
        "          internal.append(int(digits[i]))\n",
        "      return(internal)\n",
        "\n",
        "  scan_number = [0]*len(tdportal['File Name'])\n",
        "  td_samples = []\n",
        "\n",
        "  for i in range(0, len(tdportal['File Name'])):\n",
        "      scan_number[i] = str_to_int(str(tdportal['Fragment Scans'][i]))\n",
        "      if tdportal['File Name'][i] not in td_samples:\n",
        "        td_samples.append(tdportal['File Name'][i])\n",
        "\n",
        "  my_dic_scan = {key: [] for key in td_samples}\n",
        "  my_dic_index = {key: [] for key in td_samples}\n",
        "\n",
        "  for i in range(0, len(tdportal['File Name'])):\n",
        "      my_dic_scan[tdportal['File Name'][i]].append(scan_number[i])\n",
        "      my_dic_index[tdportal['File Name'][i]].append([i]*len(scan_number[i]))\n",
        "\n",
        "  for i in range(0, len(td_samples)):\n",
        "      nested_list = my_dic_scan[td_samples[i]]\n",
        "      flat_list = []\n",
        "      for item in nested_list:\n",
        "          if isinstance(item, list):\n",
        "              flat_list.extend(item)\n",
        "          else:\n",
        "              flat_list.append(item)\n",
        "      my_dic_scan[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "\n",
        "\n",
        "  for i in range(0, len(td_samples)):\n",
        "      nested_list = my_dic_index[td_samples[i]]\n",
        "      flat_list = []\n",
        "      for item in nested_list:\n",
        "          if isinstance(item, list):\n",
        "              flat_list.extend(item)\n",
        "          else:\n",
        "              flat_list.append(item)\n",
        "      my_dic_index[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "\n",
        "  sequence = []\n",
        "  MASS = []\n",
        "  Uniprot_ID = []\n",
        "  Accession = []\n",
        "\n",
        "  for i in tqdm(range(0, len(databank['scan'])), desc=\"Processing scans\", ncols=100):\n",
        "      if databank['scan'][i] in my_dic_scan[databank['sample_name'][i]]:\n",
        "          tt = my_dic_index[databank['sample_name'][i]][my_dic_scan[databank['sample_name'][i]].index(databank['scan'][i])]\n",
        "          sequence.append(tdportal['Sequence'][tt])\n",
        "          MASS.append(tdportal['Average Mass'][tt])\n",
        "          Uniprot_ID.append(tdportal['Uniprot Id'][tt])\n",
        "          Accession.append(tdportal['Accession'][tt])\n",
        "      else:\n",
        "          sequence.append('None')\n",
        "          MASS.append('None')\n",
        "          Uniprot_ID.append('None')\n",
        "          Accession.append('None')\n",
        "\n",
        "\n",
        "  databank['sequence'] = sequence\n",
        "  databank['MASS'] = MASS\n",
        "  databank['Uniprot ID'] = Uniprot_ID\n",
        "  databank['Accession'] = Accession\n",
        "\n",
        "  databank = pd.DataFrame(databank)\n",
        "\n",
        "  databank.to_hdf(cast_path, key=\"databank\", mode=\"w\")\n",
        "\n",
        "  return()\n",
        ""
      ],
      "metadata": {
        "id": "wxih8rt18wJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data piepline to build Databank"
      ],
      "metadata": {
        "id": "2oTQP93UvWeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wholeCasting(\"D:/samples/\",'D:/final/databank')"
      ],
      "metadata": {
        "id": "GUT25pxJkovu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data piepline to incorporate tdportal report to Databank"
      ],
      "metadata": {
        "id": "CFVD8zrc9WvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"D:/final/databank\", \"rb\") as f:\n",
        "    databank = pickle.load(f)\n",
        "tdportal = pd.read_csv('D:/final/tdreport.csv')\n",
        "cast_path = 'D:/final/databank_updated'\n",
        "\n",
        "ID_import(tdportal, databank, cast_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "VvorIGdngMtG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}