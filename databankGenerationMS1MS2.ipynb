{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1X8yo2UI7EEgTRg11f7GTaA_D2F-AFQkc",
      "authorship_tag": "ABX9TyOLsp1D2ntSqi2O8oEaL+Xi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminnigjeh/keyProteoforms/blob/main/databankGenerationMS1MS2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Install Fisher-py module\n",
        "This is a module for parsing Thermo raw files. Since it's run time is pythonnet, it requires installation of .NET framework."
      ],
      "metadata": {
        "id": "6QCFIUVefrKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y mono-complete\n",
        "!pip install fisher-py"
      ],
      "metadata": {
        "id": "vJ1Z4iWWe42d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import external libraries"
      ],
      "metadata": {
        "id": "l19s1jqSgcdF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fisher_py.data.business import Scan\n",
        "from fisher_py import RawFile\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle"
      ],
      "metadata": {
        "id": "aFX1YSz_c-sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Function to build MS2 Databank"
      ],
      "metadata": {
        "id": "3R99nK-8j4dE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wholeCasting(folder_path, cast_path, group_name):\n",
        "    os.chdir(folder_path)\n",
        "\n",
        "    def helper_regex(text):\n",
        "        match = re.search(rf\"{'Full'}\\s+(\\w+)\", text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        return None\n",
        "    def find_matching_keys(sequence: str, substring_dict: dict) -> list:\n",
        "        return [key for key, substrings in substring_dict.items() if any(substring in sequence for substring in substrings)]\n",
        "\n",
        "\n",
        "    files_1 = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "    substring_dict_sample = {\"Heart\": [\"Heart\"], \"Kidney\": [\"Kidney\"], \"Spleen\": [\"Spleen\"], \"Lung\": [\"Lung\"], \"SmInt\": [\"SmInt\"] }\n",
        "    substring_dict_elution = {\"PLRPS\": [\"PLRPS\"], \"CESI\": [\"CESI\"]}\n",
        "\n",
        "    files = [s for s in files_1 if group_name in s]\n",
        "\n",
        "    file_name = []\n",
        "    sample_group = []\n",
        "    elution_group = []\n",
        "\n",
        "    scan_type = []\n",
        "    scan_number = []\n",
        "    retention_time = []\n",
        "    cast_spectra = []\n",
        "\n",
        "    mz_value = []\n",
        "\n",
        "    for raw_name in files:\n",
        "        raw = RawFile(raw_name)\n",
        "        print(raw_name)\n",
        "        for i in tqdm(range(1, raw.number_of_scans), desc=\"Processing scans\", ncols=100):\n",
        "            raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
        "\n",
        "            if str(helper_regex(raw_scan.scan_type)) == 'ms2':\n",
        "                file_name.append(raw_name)\n",
        "                sample_group.append(find_matching_keys(raw_name, substring_dict_sample)[0])\n",
        "                elution_group.append(find_matching_keys(raw_name, substring_dict_elution)[0])\n",
        "                scan_type.append('MS2')\n",
        "                scan_number.append(raw_scan.scan_statistics.scan_number)\n",
        "                retention_time.append(raw.get_retention_time_from_scan_number(raw_scan.scan_statistics.scan_number))\n",
        "                mz_value.append(float(re.findall(r'[\\d]*[.][\\d]+', raw_scan.scan_type)[1]))\n",
        "\n",
        "                data_intensities = [0]*1600\n",
        "                scan_masses = raw_scan.preferred_masses\n",
        "                scan_intensities = raw_scan.preferred_intensities\n",
        "\n",
        "                for j in range(0,len(scan_masses)):\n",
        "                    index = round(scan_masses[j])\n",
        "                    if index > 400 and index < 2000:\n",
        "                        data_intensities[index-400] = scan_intensities[j] + data_intensities[index-400]\n",
        "                data_intensities = np.array(data_intensities)\n",
        "                max_value = np.max(data_intensities)\n",
        "                data_intensities_norm = data_intensities / max_value\n",
        "                data_intensities_norm = data_intensities_norm.astype(np.float16)\n",
        "                data_intensities_norm.tolist()\n",
        "                cast_spectra.append(data_intensities_norm)\n",
        "\n",
        "    scan_dict = {'sample_name': file_name, 'group_name': sample_group, 'separation': elution_group, 'scan': scan_number,'scan_type': scan_type, 'retntion time': retention_time, 'm/z': mz_value, 'cast spectra': cast_spectra}\n",
        "\n",
        "    with open(cast_path, \"wb\") as f:\n",
        "        pickle.dump(scan_dict, f)\n",
        "\n",
        "    return()"
      ],
      "metadata": {
        "id": "D-Xxi4bH9qvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper Function to incorporate tdporal report into MS2 Databank"
      ],
      "metadata": {
        "id": "xDKk8aLD830V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ID_import(tdportal, databank, cast_path):\n",
        "  def str_to_int(st):\n",
        "      internal = []\n",
        "      digits = re.findall(r'\\d+', st)\n",
        "      for i in range(0, len(digits)):\n",
        "          internal.append(int(digits[i]))\n",
        "      return(internal)\n",
        "\n",
        "  scan_number = [0]*len(tdportal['File Name'])\n",
        "  td_samples = []\n",
        "\n",
        "  for i in range(0, len(tdportal['File Name'])):\n",
        "      scan_number[i] = str_to_int(str(tdportal['Fragment Scans'][i]))\n",
        "      if tdportal['File Name'][i] not in td_samples:\n",
        "        td_samples.append(tdportal['File Name'][i])\n",
        "\n",
        "  my_dic_scan = {key: [] for key in td_samples}\n",
        "  my_dic_index = {key: [] for key in td_samples}\n",
        "\n",
        "  for i in range(0, len(tdportal['File Name'])):\n",
        "      my_dic_scan[tdportal['File Name'][i]].append(scan_number[i])\n",
        "      my_dic_index[tdportal['File Name'][i]].append([i]*len(scan_number[i]))\n",
        "\n",
        "  for i in range(0, len(td_samples)):\n",
        "      nested_list = my_dic_scan[td_samples[i]]\n",
        "      flat_list = []\n",
        "      for item in nested_list:\n",
        "          if isinstance(item, list):\n",
        "              flat_list.extend(item)\n",
        "          else:\n",
        "              flat_list.append(item)\n",
        "      my_dic_scan[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "\n",
        "\n",
        "  for i in range(0, len(td_samples)):\n",
        "      nested_list = my_dic_index[td_samples[i]]\n",
        "      flat_list = []\n",
        "      for item in nested_list:\n",
        "          if isinstance(item, list):\n",
        "              flat_list.extend(item)\n",
        "          else:\n",
        "              flat_list.append(item)\n",
        "      my_dic_index[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
        "\n",
        "  sequence = []\n",
        "  MASS = []\n",
        "  Uniprot_ID = []\n",
        "  Accession = []\n",
        "  Modifications = []\n",
        "\n",
        "  for i in tqdm(range(0, len(databank['scan'])), desc=\"Processing scans\", ncols=100):\n",
        "        if databank['sample_name'][i] in my_dic_scan.keys():\n",
        "            if databank['scan'][i] in my_dic_scan[databank['sample_name'][i]]:\n",
        "                tt = my_dic_index[databank['sample_name'][i]][my_dic_scan[databank['sample_name'][i]].index(databank['scan'][i])]\n",
        "                sequence.append(tdportal['Sequence'][tt])\n",
        "                MASS.append(tdportal['Average Mass'][tt])\n",
        "                Uniprot_ID.append(tdportal['Uniprot Id'][tt])\n",
        "                Accession.append(tdportal['Accession'][tt])\n",
        "                Modifications.append(tdportal['Modifications'][tt] if pd.notna(tdportal['Modifications'][tt]) else \"None\")\n",
        "            else:\n",
        "                sequence.append('None')\n",
        "                MASS.append('None')\n",
        "                Uniprot_ID.append('None')\n",
        "                Accession.append('None')\n",
        "                Modifications.append('None')\n",
        "        else:\n",
        "            sequence.append('None')\n",
        "            MASS.append('None')\n",
        "            Uniprot_ID.append('None')\n",
        "            Accession.append('None')\n",
        "            Modifications.append('None')\n",
        "\n",
        "\n",
        "  databank['sequence'] = sequence\n",
        "  databank['MASS'] = MASS\n",
        "  databank['Uniprot ID'] = Uniprot_ID\n",
        "  databank['Accession'] = Accession\n",
        "  databank['Modifications'] = Modifications\n",
        "\n",
        "  with open(cast_path, \"wb\") as f:\n",
        "        pickle.dump(databank, f)\n",
        "\n",
        "\n",
        "  return()"
      ],
      "metadata": {
        "id": "wxih8rt18wJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data piepline to build MS2 Databank"
      ],
      "metadata": {
        "id": "2oTQP93UvWeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wholeCasting(\"E:/raw/samples\",'E:/raw/Heart', \"Heart\")\n",
        "wholeCasting(\"E:/raw/samples\",'E:/raw/Kidney', \"Kidney\")\n",
        "wholeCasting(\"E:/raw/samples\",'E:/raw/Lung', \"Lung\")\n",
        "wholeCasting(\"E:/raw/samples\",'E:/raw/Spleen', \"Spleen\")\n",
        "wholeCasting(\"E:/raw/samples\",'E:/raw/SmInt', \"SmInt\")"
      ],
      "metadata": {
        "id": "GUT25pxJkovu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data piepline to incorporate tdportal report to MS2 Databank"
      ],
      "metadata": {
        "id": "CFVD8zrc9WvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"E:/raw/SmInt\", \"rb\") as f:\n",
        "    databank = pickle.load(f)\n",
        "tdportal = pd.read_csv('E:/raw/SmInt.csv')\n",
        "cast_path = 'E:/raw/SmInt_updated'\n",
        "\n",
        "ID_import(tdportal, databank, cast_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "VvorIGdngMtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Combine multiple MS2 databanks"
      ],
      "metadata": {
        "id": "T_Js19B1-6sC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "folder_path = \"E:/raw/databank\"  # Change this to your folder path\n",
        "output_pickle = \"E:/raw/combined_data.pkl\"\n",
        "\n",
        "# List all pickle files in the folder\n",
        "files = [f for f in os.listdir(folder_path)]\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for file in files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "\n",
        "    # Load pickled dictionary\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        data_dict = pickle.load(f)\n",
        "\n",
        "    # Convert dictionary to DataFrame (assuming it's structured correctly)\n",
        "    df = pd.DataFrame(data_dict)\n",
        "\n",
        "    df_list.append(df)\n",
        "\n",
        "# Combine all DataFrames\n",
        "combined_df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Pickle the final DataFrame\n",
        "combined_df.to_pickle(output_pickle)\n",
        "\n",
        "print(f\"Combined DataFrame saved to {output_pickle}\")\n"
      ],
      "metadata": {
        "id": "25M_iA1r-4ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load databank"
      ],
      "metadata": {
        "id": "DdX3sLjG_AVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_loaded = pd.read_pickle('E:/raw/combined_data.pkl')\n",
        "databank = pd.DataFrame(df_loaded)"
      ],
      "metadata": {
        "id": "f-OhFGSp-_-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper functions to build MS1 databank"
      ],
      "metadata": {
        "id": "bYJ3azn1_Xj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MS1wholeCasting(file_name):\n",
        "\n",
        "\n",
        "    def helper_regex(text):\n",
        "        match = re.search(rf\"{'Full'}\\s+(\\w+)\", text)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        return None\n",
        "    def find_matching_keys(sequence: str, substring_dict: dict) -> list:\n",
        "        return [key for key, substrings in substring_dict.items() if any(substring in sequence for substring in substrings)]\n",
        "\n",
        "\n",
        "\n",
        "    substring_dict_sample = {\"Heart\": [\"Heart\"], \"Kidney\": [\"Kidney\"], \"Spleen\": [\"Spleen\"], \"Lung\": [\"Lung\"], \"SmInt\": [\"SmInt\"] }\n",
        "    substring_dict_elution = {\"PLRPS\": [\"PLRPS\"], \"CESI\": [\"CESI\"]}\n",
        "\n",
        "\n",
        "\n",
        "    file_name1 = []\n",
        "    sample_group = []\n",
        "    elution_group = []\n",
        "\n",
        "    scan_type = []\n",
        "    scan_number = []\n",
        "    retention_time = []\n",
        "    cast_spectra = []\n",
        "\n",
        "\n",
        "    raw = RawFile(file_name)\n",
        "\n",
        "    for i in tqdm(range(1, raw.number_of_scans), desc=\"Processing scans\", ncols=100):\n",
        "        raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
        "\n",
        "        if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
        "            file_name1.append(file_name)\n",
        "            sample_group.append(find_matching_keys(file_name, substring_dict_sample)[0])\n",
        "            elution_group.append(find_matching_keys(file_name, substring_dict_elution)[0])\n",
        "            retention_time.append(raw.get_retention_time_from_scan_number(raw_scan.scan_statistics.scan_number))\n",
        "            scan_type.append('MS1')\n",
        "            scan_number.append(raw_scan.scan_statistics.scan_number)\n",
        "            retention_time.append(raw.get_retention_time_from_scan_number(raw_scan.scan_statistics.scan_number))\n",
        "\n",
        "            data_intensities = [0]*13690\n",
        "            scan_masses = raw_scan.preferred_masses\n",
        "            scan_intensities = raw_scan.preferred_intensities\n",
        "\n",
        "            for j in range(0,len(scan_masses)):\n",
        "                index = int(round(scan_masses[j], 2)*10)\n",
        "                if index > 6000 and index < 19360:\n",
        "                    data_intensities[index-6000] = scan_intensities[j] + data_intensities[index-6000]\n",
        "\n",
        "            cast_spectra.append(data_intensities)\n",
        "\n",
        "    scan_dict = {'sample_name': file_name, 'group_name': sample_group, 'separation': elution_group, 'scan': scan_number,'scan_type': scan_type, 'retntion time': retention_time, 'cast spectra': cast_spectra}\n",
        "\n",
        "    with open(file_name[:-4], \"wb\") as f:\n",
        "        pickle.dump(scan_dict, f)\n",
        "\n",
        "    return()"
      ],
      "metadata": {
        "id": "8_k6GvLp-CtW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}