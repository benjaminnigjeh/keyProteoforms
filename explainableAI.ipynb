{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM02oEX4KUaOGzaN1jzbID+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminnigjeh/keyProteoforms/blob/main/explainableAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install -y -qq mono-complete\n",
        "!pip install -qq fisher-py"
      ],
      "metadata": {
        "id": "Sadvp9OJ0dDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import external libraries"
      ],
      "metadata": {
        "id": "VUDgxCFdB488"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fisher_py.data.business import Scan\n",
        "from fisher_py import RawFile\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import subprocess\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "from scipy.signal import savgol_filter\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "GJFZsw6N0J7e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Helper functions"
      ],
      "metadata": {
        "id": "xWnlt5yNB-zp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def helper_regex(text):\n",
        "    match = re.search(rf\"{'Full'}\\s+(\\w+)\", text)\n",
        "    if match:\n",
        "        return match.group(1)\n",
        "    return None\n",
        "\n",
        "def MS1Casting(folder_path, file_path):\n",
        "    os.chdir(folder_path)\n",
        "    raw = RawFile(file_path)\n",
        "    data_intensities = [0]*1369\n",
        "    for i in tqdm(range(1, raw.number_of_scans)):\n",
        "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
        "                if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
        "                        scan_masses = raw_scan.preferred_masses\n",
        "                        scan_intensities = raw_scan.preferred_intensities\n",
        "                        for j in range(0,len(scan_masses)):\n",
        "                                index = round(scan_masses[j])\n",
        "                                if index > 600 and index < 1969:\n",
        "                                        data_intensities[index-600] = scan_intensities[j] + data_intensities[index-600]\n",
        "    return(data_intensities)\n",
        "\n",
        "def MS1Casting_highres(folder_path, file_path):\n",
        "    os.chdir(folder_path)\n",
        "    raw = RawFile(file_path)\n",
        "    data_intensities = [0]*13690\n",
        "    for i in tqdm(range(1, raw.number_of_scans)):\n",
        "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
        "                if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
        "                        scan_masses = raw_scan.preferred_masses\n",
        "                        scan_intensities = raw_scan.preferred_intensities\n",
        "                        for j in range(0,len(scan_masses)):\n",
        "                                index = int((round(scan_masses[j], 1))*10)\n",
        "                                if index > 6000 and index < 19690:\n",
        "                                        data_intensities[index-6000] = scan_intensities[j] + data_intensities[index-6000]\n",
        "    return(data_intensities)\n",
        "\n",
        "def deconvolute(target_list, intensity, i):\n",
        "    masses = []\n",
        "    intensities = []\n",
        "\n",
        "    for target in target_list:\n",
        "        for m, inten in zip(mass, intensity):\n",
        "            if m > (target - 5) and m < (target + 5):\n",
        "                masses.append(m)\n",
        "                intensities.append(inten)\n",
        "\n",
        "    data = {\n",
        "                'Column1': masses,\n",
        "                'Column2': intensities}\n",
        "    df = pd.DataFrame(data)\n",
        "    file_name = 'D:/' + str(i) + 'temp.txt'\n",
        "    df.to_csv(file_name, sep='\\t', index=False)\n",
        "    command = f\"python -m unidec -f {file_name}\"\n",
        "    subprocess.run(command, shell=True)\n",
        "    return()"
      ],
      "metadata": {
        "id": "Wm1QrIddBRFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate high-resolution dataset"
      ],
      "metadata": {
        "id": "8HoVj-bWCELz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = 'D:/conv/samples/'\n",
        "\n",
        "os.chdir(folder_path)\n",
        "\n",
        "file_path_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
        "\n",
        "result = []\n",
        "\n",
        "for file_path in file_path_list:\n",
        "    print(file_path)\n",
        "    folder_path = 'D:/conv/samples/'\n",
        "    result.append(MS1Casting_highres(folder_path, file_path))\n",
        "\n",
        "df = pd.DataFrame(result)\n",
        "\n",
        "column_max = df.max()\n",
        "\n",
        "df_normalized = df / (column_max + 0.001)\n",
        "\n",
        "df_normalized['target'] = file_path_list\n",
        "\n",
        "df_normalized.to_csv('D:/res.csv')"
      ],
      "metadata": {
        "id": "hjvv3-7TBU9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import dataset"
      ],
      "metadata": {
        "id": "ZsSy0F3SCMF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = files.upload_file('dataset')\n",
        "df = pd.read_csv('/content/dataset')\n",
        "\n",
        "X = df.copy()\n",
        "Y = X.pop(\"target\")\n",
        "X = np.array(X)\n",
        "y = np.array(Y)"
      ],
      "metadata": {
        "id": "sLEmtLqhBazy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=20)\n",
        "\n",
        "# Replace NaNs with zero in both features and targets\n",
        "X_train = np.nan_to_num(X_train)\n",
        "X_val = np.nan_to_num(X_val)\n",
        "y_train = np.nan_to_num(y_train)\n",
        "y_val = np.nan_to_num(y_val)"
      ],
      "metadata": {
        "id": "eHOFcwzRBhE7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(128, input_dim=13690, activation='relu',\n",
        "               kernel_regularizer=regularizers.l1(0.01)))  # L1 regularization\n",
        "model.add(Dense(32, activation='relu'))  # Hidden layer with 32 units\n",
        "model.add(Dense(1, activation='sigmoid'))  # Output layer with a single neuron for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSk_zj8CBlbE",
        "outputId": "139381dd-c629-4e73-a90a-bc542db6d435"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model and store history to plot binary cross-entropy\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Plot binary cross-entropy loss per epoch\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Binary Cross-Entropy Loss vs Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Binary Cross-Entropy Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LYCxNjMmBtxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **Gradient-Based Method (Saliency Map)**\n",
        "@tf.function\n",
        "def compute_gradients(input_sample):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(input_sample)  # Watch the input\n",
        "        predictions = model(input_sample, training=False)  # Make predictions\n",
        "    gradients = tape.gradient(predictions, input_sample)  # Compute gradients\n",
        "    return gradients\n",
        "\n",
        "# Compute gradients for the first sample in the test set\n",
        "sample_input = tf.convert_to_tensor(X_train[:1], dtype=tf.float32)\n",
        "gradients = compute_gradients(sample_input)\n",
        "\n",
        "# Calculate the absolute sum of gradients for each feature\n",
        "importance_gradients = np.abs(gradients.numpy()).sum(axis=0)\n",
        "\n",
        "# Sort features by their importance based on gradients\n",
        "sorted_features_gradients = np.argsort(importance_gradients)[::-1]\n",
        "\n",
        "# Output feature importance based on gradients\n",
        "print(\"\\n--- Feature Importance Based on Gradients ---\")\n",
        "for i in sorted_features_gradients:\n",
        "    print(f\"Feature {i} Gradient Importance: {importance_gradients[i]}\")"
      ],
      "metadata": {
        "id": "KQrsYVqhI_MW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'D:/soluble_weights_800.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "x = np.array(data['800'])\n",
        "y = np.array(data['9.34E-06'])"
      ],
      "metadata": {
        "id": "2jGbIyjPJC4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply baseline correction (subtract the baseline)\n",
        "# Using Savitzky-Golay filter to remove the baseline trend\n",
        "baseline = savgol_filter(y, window_length=10000, polyorder=3)  # Smoothing the signal\n",
        "corrected_y = y - baseline # Subtract baseline to remove it\n",
        "\n",
        "# Apply a 5-point moving average to the corrected signal to get the trend line\n",
        "trend_line = np.convolve(corrected_y, np.ones(20)/20, mode='valid')\n",
        "\n",
        "# from scipy.ndimage import gaussian_filter1d\n",
        "\n",
        "# trend_line = gaussian_filter1d(trend_line, sigma=20)\n",
        "\n",
        "# from scipy.signal import savgol_filter\n",
        "\n",
        "# trend_line = savgol_filter(trend_line, window_length=11, polyorder=2)\n",
        "\n",
        "# Define the minimum peak height (this is the threshold for peak detection)\n",
        "min_peak_height = 0.5  # Change this value based on your signal\n",
        "\n",
        "# Find the peaks in the trend line with the defined height threshold\n",
        "peaks, _ = find_peaks(trend_line, height=min_peak_height, distance=20)\n",
        "\n",
        "# Filter peaks with y-values less than 1600\n",
        "filtered_peaks = peaks[trend_line[peaks] < 1600]\n",
        "\n",
        "# Plot the data and the detected peaks\n",
        "plt.plot(x[:len(trend_line)], trend_line, label='Trend Line (Baseline Corrected)')\n",
        "plt.plot(x[filtered_peaks], trend_line[filtered_peaks], \"x\", label='Peaks < 1600')\n",
        "plt.xlabel('X')\n",
        "plt.xlim([600, 1600])\n",
        "plt.ylabel('Y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Output the filtered peak values\n",
        "peak_x_values = x[filtered_peaks]\n",
        "peak_y_values = trend_line[filtered_peaks]\n",
        "print(f\"Filtered Peak X values (less than 1600): {peak_x_values}\")\n",
        "print(f\"Filtered Peak Y values (less than 1600): {peak_y_values}\")\n"
      ],
      "metadata": {
        "id": "1aVlKC-TJsNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.DataFrame(trend_line)\n",
        "a.to_csv('D:/trendline.csv')"
      ],
      "metadata": {
        "id": "oRDJYDuqJ5ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = list(peak_x_values)\n",
        "target_list = [num for num in target if num > 800]"
      ],
      "metadata": {
        "id": "_QXANBFXJ8v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('D:/res_soluble_sorted.csv')\n",
        "X = df.copy()\n",
        "Y = X.pop(\"target\")\n",
        "X = np.array(X)\n",
        "y = np.array(Y)\n",
        "\n",
        "mass = [round(x * 0.1, 1) for x in range(6000, 19690)]\n",
        "\n",
        "intensity = list(X[0])"
      ],
      "metadata": {
        "id": "jH0UC3iWKM4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disease = [0] * len(X[0])  # Initialize the disease list for summing\n",
        "healthy = [0] * len(X[0])  # Initialize the healthy list for summing\n",
        "\n",
        "# Iterate through all entries in y\n",
        "for i in range(len(y)):\n",
        "    if y[i] == 1:\n",
        "        # Add features to the disease group\n",
        "        disease = [x + feature for x, feature in zip(disease, X[i])]\n",
        "    elif y[i] == 0:\n",
        "        # Add features to the healthy group\n",
        "        healthy = [x + feature for x, feature in zip(healthy, X[i])]\n",
        "\n",
        "# Now `disease` and `healthy` hold the sum of features for each group\n"
      ],
      "metadata": {
        "id": "JtHidH4KKOHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deconvolute(target_list, disease, 1)\n",
        "deconvolute(target_list, healthy, 0)"
      ],
      "metadata": {
        "id": "xgoEJ366KTuK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}