{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1pDeKC59T7sG5fy1AkmfF7vgiwOrySf1s",
      "authorship_tag": "ABX9TyMXOz11KMqxgg3sTVQ3dS9O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjaminnigjeh/keyProteoforms/blob/main/generalized_similarity_search_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdpIHDgVwsrz"
      },
      "outputs": [],
      "source": [
        "import hashlib\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import ast\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "import pickle\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "random.seed(100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MinHash:\n",
        "    def __init__(self, num_hashes=100):\n",
        "        self.num_hashes = num_hashes\n",
        "        self.seeds = [random.randint(0, 2**32 - 1) for _ in range(num_hashes)]\n",
        "\n",
        "    def _hash(self, x, seed):\n",
        "        return int(hashlib.md5((str(seed) + x).encode('utf8')).hexdigest(), 16)\n",
        "\n",
        "    def compute(self, set_data):\n",
        "        min_hashes = [min(self._hash(el, seed) for el in set_data) for seed in self.seeds]\n",
        "        return min_hashes\n",
        "\n",
        "    def jaccard_similarity(self, set1, set2):\n",
        "        min_hash1 = self.compute(set1)\n",
        "        min_hash2 = self.compute(set2)\n",
        "        return sum(1 for a, b in zip(min_hash1, min_hash2) if a == b) / self.num_hashes\n",
        "\n",
        "\n",
        "def hashing(str1, str2):\n",
        "\n",
        "    n = 3\n",
        "    set1 = {str1[i:i+n] for i in range(len(str1) - n + 1)}\n",
        "    set2 = {str2[i:i+n] for i in range(len(str2) - n + 1)}\n",
        "    minhash = MinHash(num_hashes=100)\n",
        "    similarity = minhash.jaccard_similarity(set1, set2)\n",
        "    return(1 - similarity)\n",
        "\n",
        "def plot_distribution(data):\n",
        "    if isinstance(data, list):\n",
        "        data = np.array(data)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    sns.histplot(data, bins=30, kde=True, color='blue', alpha=0.6, stat='density')\n",
        "\n",
        "    plt.xlabel('Value')\n",
        "    plt.ylabel('Density')\n",
        "    plt.title('Distribution of Numbers')\n",
        "\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix(model, X_test, y_test):\n",
        "  y_true = [0 if x < 0.5 else 1 for x in y_test]\n",
        "  y_pred = [0 if x < 0.5 else 1 for x in model.predict(X_test)]\n",
        "\n",
        "  cm = confusion_matrix(y_true, y_pred)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y_true))\n",
        "  disp.plot(cmap=plt.cm.Blues)\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "vI8gP6OomwbG"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8fjp_g7kmf78"
      },
      "outputs": [],
      "source": [
        "databank_path = '/content/drive/MyDrive/databank.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(databank_path, 'rb') as f:\n",
        "  databank = pickle.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ned_C_MfmtzZ",
        "outputId": "6909107b-5159-4fc0-800c-94469ff3f147"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-3001639f3231>:2: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
            "  databank = pickle.load(f)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "databank.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmhWlbeem2eb",
        "outputId": "28a3949a-f4c4-4d3e-9069-c5fd6ab356d3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sample_name', 'group_name', 'separation', 'scan', 'scan_type',\n",
              "       'retntion time', 'm/z', 'cast spectra', 'sequence', 'MASS',\n",
              "       'Uniprot ID', 'Accession', 'Modifications'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = databank['Uniprot ID'].unique()\n",
        "b = databank['Uniprot ID'].value_counts().reindex(a, fill_value=0).tolist()\n",
        "\n",
        "sorted_accessions = sorted(zip(a, b), key=lambda x: x[1], reverse=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "6GxFJfU6oNgk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_accessions[1][0]"
      ],
      "metadata": {
        "id": "lBjaGkq9qWuC",
        "outputId": "4683001e-1755-4999-9010-c23cfb814b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HBB_HUMAN'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(41):\n",
        "  print(sorted_accessions[i][1])"
      ],
      "metadata": {
        "id": "G70bpiyksNJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = databank.copy()"
      ],
      "metadata": {
        "id": "YAiQHHX-KaG2"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id = []\n",
        "sequence = []\n",
        "cast = []\n",
        "\n",
        "for i in range(0, len(df['scan'])):\n",
        "    if df['scan_type'][i] == 'MS2':\n",
        "        if df['Uniprot ID'][i] != 'None':\n",
        "            id.append(df['Uniprot ID'][i])\n",
        "            sequence.append(df['sequence'][i])\n",
        "            cast.append(df['cast spectra'][i])\n",
        "\n",
        "ref_seq = 'MDVFMKGLSKAKEGVVAAAEKTKQGVAEAAGKTKEGVLYVGSKTKEGVVHGVATVAEKTKEQVTNVGGAVVTGVTAVAQKTVEGAGSIAAATGFVKKDQLGKNEEGAPQEGILEDMPVDPDNEAYEMPSEEGYQDYEPEA'"
      ],
      "metadata": {
        "id": "oHb-jjkAKU6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cast_flat = []\n",
        "hash = []\n",
        "\n",
        "for i in range(0, len(sequence)):\n",
        "    cast_flat.append(cast[i])\n",
        "    hash.append(hashing(sequence[i], ref_seq))\n",
        "\n",
        "X = np.array(cast_flat)\n",
        "y = np.array(hash)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=100)\n"
      ],
      "metadata": {
        "id": "hFKuEaqpKdY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(1600,)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Train the model and store training history\n",
        "history = model.fit(X_train, y_train, epochs=40, batch_size=512, validation_split=0.2)\n",
        "\n",
        "# Evaluate on test data\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
        "\n",
        "# ðŸŽ¨ Plot loss curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ECW7JMD2KjUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(model, X_test, y_test):\n",
        "    # Ensure y_test is binary (0 or 1)\n",
        "    #y_test = np.array(y_test).astype(int)  # âœ… Convert to integer if needed\n",
        "    y_test = np.where(np.array(y_test) > 0.5, 1, 0).astype(int)\n",
        "\n",
        "    # Get model predictions\n",
        "    y_scores = model.predict(X_test)\n",
        "\n",
        "    # Convert predictions to binary labels\n",
        "    y_pred = (y_scores >= 0.5).astype(int).flatten()  # âœ… Convert float â†’ int\n",
        "\n",
        "    # Ensure shapes match\n",
        "    if y_test.shape != y_pred.shape:\n",
        "        print(f\"Shape mismatch: y_test {y_test.shape}, y_pred {y_pred.shape}\")\n",
        "        y_test = y_test.flatten()\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Display the confusion matrix\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "plot_confusion_matrix(model, X_test, y_test)"
      ],
      "metadata": {
        "id": "XGm01-SSKnvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Generate sample data\n",
        "data = np.array(hash)  # Normally distributed data\n",
        "\n",
        "# Plot the frequency distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data, bins=100, kde=True, color='blue', legend=False)\n",
        "\n",
        "#plt.yscale('log', base=2)\n",
        "# Labels and title\n",
        "plt.xlabel('Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency Distribution')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wY2kgZowKs_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cast = []\n",
        "\n",
        "for i in range(0, len(df['scan'])):\n",
        "    if df['scan_type'][i] == 'MS2':\n",
        "          cast.append(df['cast spectra'][i])\n",
        "\n",
        "hash = model.predict(np.array(cast))\n"
      ],
      "metadata": {
        "id": "NvZDcLrVKyDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(cast)\n",
        "y = np.array(hash)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=100)\n"
      ],
      "metadata": {
        "id": "QxKxlaFUK3Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cast = []\n",
        "\n",
        "for i in range(0, len(df['scan'])):\n",
        "    if df['scan_type'][i] == 'MS2':\n",
        "      if df['Uniprot ID'][i] == 'None':\n",
        "          cast.append(df['cast spectra'][i])\n",
        "\n",
        "hash = model.predict(np.array(cast))\n",
        "\n",
        "a = 0\n",
        "b = 0\n",
        "\n",
        "for i in range(0, len(hash)):\n",
        "  if hash[i] > 0.5:\n",
        "    a += 1\n",
        "  else:\n",
        "    b += 1\n",
        "\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "id": "xg1vCdN9K8Dg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}